{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from SynthTempNetwork import Individual, SynthTempNetwork\n",
    "from TemporalNetwork import ContTempNetwork, StaticTempNetwork\n",
    "from FlowStability import SparseClustering, FlowIntegralClustering, run_multi_louvain, avg_norm_var_information\n",
    "import parallel_clustering\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotx\n",
    "\n",
    "from scipy.sparse import (lil_matrix, dok_matrix, diags, eye, isspmatrix_csr, isspmatrix,\n",
    "                          csr_matrix, coo_matrix, csc_matrix)\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "from SparseStochMat import sparse_autocov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_rw = ContTempNetwork.load('/home/b/skoove/Desktop/entropy/paper_data/socio_pat_primary_school/primaryschoolnet',\n",
    "                              attributes_list=['node_to_label_dict',\n",
    "                      'events_table',\n",
    "                      'times',\n",
    "                      'time_grid',\n",
    "                      'num_nodes',\n",
    "                      '_overlapping_events_merged',\n",
    "                      'start_date',\n",
    "                      'node_label_array',\n",
    "                      'male_array',\n",
    "                      'female_array',\n",
    "                      'node_first_start_array',\n",
    "                      'node_last_end_array',\n",
    "                      'node_class_array',\n",
    "                      'datetimes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../paper_data/socio_pat_primary_school/primaryschool.csv',\n",
    "                 header=None, sep='\\t', names=['time','id1','id2','class1','class2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['time'] // 3600\n",
    "df['minute'] = (df['time'] % 3600) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_times_hours = net_rw.times / 3600\n",
    "flag10 = np.argmax(net_times_hours > 10)\n",
    "flag12 = np.argmax(net_times_hours > 12)\n",
    "flag14 = np.argmax(net_times_hours > 14)\n",
    "flag16 = np.argmax(net_times_hours > 16)\n",
    "flagday1 = np.argmax(net_times_hours > 18)\n",
    "print(flag10, flag12, flag14, flag16, flagday1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_rw.times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net_times_hours[960])\n",
    "print(net_rw.times[960])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(net_rw.compute_static_adjacency_matrix(start_time= net_rw.times[0], end_time= net_rw.times[240]).toarray())\n",
    "plt.matshow(net_rw.compute_static_adjacency_matrix(start_time= net_rw.times[240], end_time= net_rw.times[600]).toarray())\n",
    "plt.matshow(net_rw.compute_static_adjacency_matrix(start_time= net_rw.times[600], end_time= net_rw.times[960]).toarray())\n",
    "plt.colorbar()\n",
    "plt.matshow(net_rw.compute_static_adjacency_matrix(start_time= net_rw.times[960], end_time= net_rw.times[1556]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunch_break = net_rw.compute_static_adjacency_matrix(start_time= net_rw.times[600], end_time= net_rw.times[960]).toarray()\n",
    "lunch_break = np.where(lunch_break == 0, 1, lunch_break)\n",
    "lunch_break = np.log(lunch_break)\n",
    "plt.matshow(lunch_break)\n",
    "plt.colorbar()\n",
    "#plt.savefig('/home/b/skoove/Desktop/primary_school/a_matrix_lunch_break.png', format='png', dpi=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph_lunch_break = nx.Graph(lunch_break)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_connected_components(graph_lunch_break)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(c) for c in sorted(nx.connected_components(graph_lunch_break), key=len, reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure Flow stability - Hourly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "start_datetime = datetime(2009,10,1,9,0,0)\n",
    "end_datetime = datetime(2009,10,2,17,0,0)\n",
    "daterange = pd.date_range(start=start_datetime, freq='H', \n",
    "              end=end_datetime)\n",
    "\n",
    "timerange = [(date_i - net_rw.start_date).total_seconds() + net_rw.start_time for date_i in daterange]\n",
    "\n",
    "adjacencies = []\n",
    "time_slices = []\n",
    "for ts, te in zip(timerange[:-1], timerange[1:]):\n",
    "    A = net_rw.compute_static_adjacency_matrix(start_time=ts, \n",
    "                                            end_time=te).toarray()\n",
    "    if not (A == np.zeros_like(A)).all():\n",
    "        adjacencies.append((A))\n",
    "        time_slices.append((ts,te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slices[3][1] / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slices[0][0] / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_12_14 = nx.Graph(adjacencies[3] + adjacencies[4])\n",
    "nx.number_connected_components(graph_12_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlowStability import (norm_mutual_information, \n",
    "                               Partition, static_clustering,\n",
    "                               norm_var_information)\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repeat = 3\n",
    "\n",
    "ts = np.round(np.logspace(-1,1,num=20),2)\n",
    "ts = np.round(np.logspace(1,1.699,num=10),2)\n",
    "\n",
    "#%%\n",
    "static_clust_list = []\n",
    "\n",
    "for i, A in enumerate(adjacencies):\n",
    "    print(i)\n",
    "    # presents, = (A.sum(1) > 0).nonzero()\n",
    "    # A = A[presents,:][:,presents]\n",
    "    \n",
    "    static_clust_scan = dict()\n",
    "    for t in ts:\n",
    "        print(t)\n",
    "        clusts = []\n",
    "        stabs = []\n",
    "        \n",
    "        static_clust_scan[t] = dict()\n",
    "        \n",
    "        for _ in range(num_repeat):\n",
    "            \n",
    "            stat_clust = static_clustering(A, t=t, linearized=True)\n",
    "            stat_clust.find_louvain_clustering()\n",
    "    \n",
    "            clusts.append(stat_clust.partition.cluster_list)\n",
    "            stabs.append(stat_clust.compute_stability())\n",
    "            \n",
    "        static_clust_scan[t]['best_clust'] = clusts[np.argmax(stabs)]\n",
    "        \n",
    "        static_clust_scan[t]['nvarinf'] = np.mean([norm_var_information(c1,c2) for c1,c2 in combinations(clusts,2)])\n",
    "            \n",
    "        static_clust_scan[t]['avg_nclust'] = np.mean([len(c) for c in clusts])\n",
    "        \n",
    "    static_clust_list.append(static_clust_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ts:\n",
    "    print(static_clust_list[4][i]['avg_nclust'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas_growing = np.logspace(-5,0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw0_240 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot0_240/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw0_240[lamda] = cluster\n",
    "avg_nclusters_forw_rw0_240 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw0_240[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_forw_rw0_240 = [avg_norm_var_information(multi_res_rw0_240[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw240_600 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot240_600/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw240_600[lamda] = cluster\n",
    "avg_nclusters_forw_rw240_600 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw240_600[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_forw_rw240_600 = [avg_norm_var_information(multi_res_rw240_600[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw600_960 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot600_960/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw600_960[lamda] = cluster\n",
    "avg_nclusters_forw_rw600_960 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw600_960[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_forw_rw600_960 = [avg_norm_var_information(multi_res_rw600_960[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw960_1320 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot960_1320/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw960_1320[lamda] = cluster\n",
    "avg_nclusters_forw_rw960_1320 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw960_1320[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_forw_rw960_1320 = [avg_norm_var_information(multi_res_rw960_1320[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw960_1320 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot960_1320/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw960_1320[lamda] = cluster\n",
    "avg_nclusters_forw_rw960_1320 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw960_1320[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_forw_rw960_1320 = [avg_norm_var_information(multi_res_rw960_1320[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw1320_1556 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot1320_1556/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw1320_1556[lamda] = cluster\n",
    "avg_nclusters_forw_rw1320_1556 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw1320_1556[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_forw_rw1320_1556 = [avg_norm_var_information(multi_res_rw1320_1556[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2, sharex=False, figsize=(10, 7))\n",
    "\n",
    "####### First Plot\n",
    "color = 'tab:red'\n",
    "axs[0,0].plot(lamdas_growing, NVI_forw_rw0_240, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[0,0].set_xscale('log')\n",
    "axs[0,0].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[0,0].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[0,0].tick_params(axis='y', labelcolor=color)\n",
    "axs[0,0].text(-0.1,1.1, '(A): 08:30-10:00 (0-240)', transform=axs[0,0].transAxes)\n",
    "\n",
    "ax1 = axs[0,0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.plot(lamdas_growing, avg_nclusters_forw_rw0_240, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax1.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax1.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "####### Second Plot\n",
    "color = 'tab:red'\n",
    "axs[0,1].plot(lamdas_growing, NVI_forw_rw240_600, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[0,1].set_xscale('log')\n",
    "axs[0,1].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[0,1].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[0,1].tick_params(axis='y', labelcolor=color)\n",
    "axs[0,1].text(-0.1,1.1, '(B): 10:00-12:00 (240-600)', transform=axs[0,1].transAxes)\n",
    "\n",
    "ax2 = axs[0,1].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.plot(lamdas_growing, avg_nclusters_forw_rw240_600, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax2.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax2.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "####### Third Plot\n",
    "color = 'tab:red'\n",
    "axs[1,0].plot(lamdas_growing, NVI_forw_rw600_960, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[1,0].set_xscale('log')\n",
    "axs[1,0].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[1,0].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[1,0].tick_params(axis='y', labelcolor=color)\n",
    "axs[1,0].text(-0.1,1.1, '(C): 12:00-14:00 (600-960)', transform=axs[1,0].transAxes)\n",
    "\n",
    "ax3 = axs[1,0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax3.plot(lamdas_growing, avg_nclusters_forw_rw600_960, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax3.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax3.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "####### Fourth Plot\n",
    "color = 'tab:red'\n",
    "axs[1,1].plot(lamdas_growing, NVI_forw_rw960_1320, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[1,1].set_xscale('log')\n",
    "axs[1,1].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[1,1].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[1,1].tick_params(axis='y', labelcolor=color)\n",
    "axs[1,1].text(-0.1,1.1, '(D):14:00-16:00 (960-1320)', transform=axs[1,1].transAxes)\n",
    "\n",
    "ax4 = axs[1,1].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax4.plot(lamdas_growing, avg_nclusters_forw_rw960_1320, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax4.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax4.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax4.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "####### Fifth Plot\n",
    "color = 'tab:red'\n",
    "axs[2,0].plot(lamdas_growing, NVI_forw_rw1320_1556, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[2,0].set_xscale('log')\n",
    "axs[2,0].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[2,0].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[2,0].tick_params(axis='y', labelcolor=color)\n",
    "axs[2,0].text(-0.1,1.1, '(E):16:00-17:30 (1320-1556)', transform=axs[2,0].transAxes)\n",
    "\n",
    "ax5 = axs[2,0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax5.plot(lamdas_growing, avg_nclusters_forw_rw1320_1556, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax5.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax5.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax5.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"Forward Clustering Day 1\", fontsize=12)\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.subplots_adjust(top=0.88) # # Tight layout requires the title to be spaced accordingly\n",
    "#plt.savefig('/home/b/skoove/Desktop/primary_school/forward_clustering_split.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_nclusters_forw_rw0_240)\n",
    "print(avg_nclusters_forw_rw240_600)\n",
    "print(avg_nclusters_forw_rw600_960)\n",
    "print(avg_nclusters_forw_rw960_1320)\n",
    "print(avg_nclusters_forw_rw1320_1556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKWARD CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw_bw0_240 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot_bw0_240/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw_bw0_240[lamda] = cluster\n",
    "avg_nclusters_bw_rw0_240 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw_bw0_240[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_bw_rw0_240 = [avg_norm_var_information(multi_res_rw_bw0_240[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw_bw0_240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw_bw240_600 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot_bw240_600/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw_bw240_600[lamda] = cluster\n",
    "avg_nclusters_bw_rw240_600 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw_bw240_600[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_bw_rw240_600 = [avg_norm_var_information(multi_res_rw_bw240_600[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw_bw600_960 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot_bw600_960/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw_bw600_960[lamda] = cluster\n",
    "avg_nclusters_bw_rw600_960 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw_bw600_960[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_bw_rw600_960 = [avg_norm_var_information(multi_res_rw_bw600_960[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw_bw960_1556 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot_bw960_1556/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "        \n",
    "    multi_res_rw_bw960_1556[lamda] = cluster\n",
    "avg_nclusters_bw_rw960_1556 = [np.mean([len(c) for c in \\\n",
    "                   multi_res_rw_bw960_1556[lamda] if len(c)>1]) for lamda in lamdas_growing]\n",
    "\n",
    "NVI_bw_rw960_1556 = [avg_norm_var_information(multi_res_rw_bw960_1556[lamda]) for lamda in lamdas_growing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, sharex=False, figsize=(10, 5))\n",
    "\n",
    "####### First Plot\n",
    "color = 'tab:red'\n",
    "axs[0,0].plot(lamdas_growing, NVI_bw_rw0_240, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[0,0].set_xscale('log')\n",
    "axs[0,0].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[0,0].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[0,0].tick_params(axis='y', labelcolor=color)\n",
    "axs[0,0].text(-0.1,1.1, '(A): 08:30-10:00 (0-240)', transform=axs[0,0].transAxes)\n",
    "\n",
    "ax1 = axs[0,0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.plot(lamdas_growing, avg_nclusters_bw_rw0_240, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax1.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax1.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "####### Second Plot\n",
    "color = 'tab:red'\n",
    "axs[0,1].plot(lamdas_growing, NVI_bw_rw240_600, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[0,1].set_xscale('log')\n",
    "axs[0,1].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[0,1].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[0,1].tick_params(axis='y', labelcolor=color)\n",
    "axs[0,1].text(-0.1,1.1, '(B): 10:00-12:00 (240-600)', transform=axs[0,1].transAxes)\n",
    "\n",
    "ax2 = axs[0,1].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.plot(lamdas_growing, avg_nclusters_bw_rw240_600, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax2.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax2.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "####### Third Plot\n",
    "color = 'tab:red'\n",
    "axs[1,0].plot(lamdas_growing, NVI_bw_rw600_960, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[1,0].set_xscale('log')\n",
    "axs[1,0].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[1,0].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[1,0].tick_params(axis='y', labelcolor=color)\n",
    "axs[1,0].text(-0.1,1.1, '(C): 12:00-14:00 (600-960)', transform=axs[1,0].transAxes)\n",
    "\n",
    "ax3 = axs[1,0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax3.plot(lamdas_growing, avg_nclusters_bw_rw600_960, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax3.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax3.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "####### Fourth Plot\n",
    "color = 'tab:red'\n",
    "axs[1,1].plot(lamdas_growing, NVI_bw_rw960_1556, ':', color=color, label='static norm NVI')\n",
    "\n",
    "axs[1,1].set_xscale('log')\n",
    "axs[1,1].set_xlabel(r'$\\lambda$ [s]')\n",
    "axs[1,1].set_ylabel('Norm. Var. Inf.', color=color)\n",
    "axs[1,1].tick_params(axis='y', labelcolor=color)\n",
    "axs[1,1].text(-0.1,1.1, '(D):14:00-17:30 (960-1556)', transform=axs[1,1].transAxes)\n",
    "\n",
    "ax4 = axs[1,1].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax4.plot(lamdas_growing, avg_nclusters_bw_rw960_1556, ':', color=color, label='edge-centric')\n",
    "\n",
    "ax4.set_xlabel(r'$\\lambda$ [s]')\n",
    "ax4.set_ylabel('Avg. no. clusters', color=color)  # we already handled the x-label with ax1\n",
    "ax4.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"Backward Clustering Day 1\", fontsize=12)\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.subplots_adjust(top=0.88) # # Tight layout requires the title to be spaced accordingly\n",
    "#plt.savefig('/home/b/skoove/Desktop/primary_school/forward_clustering_split.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res_rw0_240 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot0_240/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "\n",
    "    multi_res_rw0_240[lamda] = cluster\n",
    "\n",
    "\n",
    "multi_res_rw_bw0_240 = {}\n",
    "for lamda in lamdas_growing:\n",
    "    with open(f'//scratch/tmp/180/skoove/primaryschoolnet_rw/clustersplot_bw0_240/cluster{lamda:.11f}', 'rb') as f:\n",
    "        cluster = pickle.load(f)\n",
    "\n",
    "    multi_res_rw_bw0_240[lamda] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_clusters(cluster_list_to_sort, cluster_list_model):\n",
    "        \n",
    "    clust_similarity_lists = []\n",
    "    for clust in cluster_list_to_sort:\n",
    "        jaccs = []\n",
    "        for class_clust in cluster_list_model:\n",
    "            jaccs.append(len(clust.intersection(class_clust))/len(clust.union(class_clust)))    \n",
    "        clust_similarity_lists.append(jaccs)\n",
    "        \n",
    "    #now sort\n",
    "    clust_similarity_matrix = np.array(clust_similarity_lists)\n",
    "    new_clust_order = []\n",
    "    all_clusts = list(range(clust_similarity_matrix.shape[0]))\n",
    "    \n",
    "    while len(new_clust_order) < len(cluster_list_to_sort):\n",
    "        for cla in range(clust_similarity_matrix.shape[1]):\n",
    "            # loop on classes and sort according to most similar\n",
    "            comm = clust_similarity_matrix[all_clusts,cla].argmax()\n",
    "            if all_clusts[comm] not in new_clust_order:\n",
    "                new_clust_order.append(all_clusts[comm])\n",
    "        for n in new_clust_order:\n",
    "            if n in all_clusts:\n",
    "                all_clusts.remove(n)\n",
    "                \n",
    "    return [cluster_list_to_sort[i] for i in new_clust_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##all data\n",
    "int_start = 0\n",
    "int_stop = 1\n",
    "\n",
    "#%% load forward and backward partitions\n",
    "\n",
    "tau_w = lamdas_growing[100]\n",
    "\n",
    "color_list =[\"#ffffff\",\n",
    "\"#4ba706\",\n",
    "\"#a2007e\",\n",
    "\"#806dcb\",\n",
    "\"#5eb275\",\n",
    "\"#ca3b01\",\n",
    "\"#01a4d6\",\n",
    "\"#b77600\",\n",
    "\"#a39643\",\n",
    "\"#cc6ea9\",\n",
    "\"#1e5e39\",\n",
    "\"#cb5b5a\"]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(color_list)\n",
    "\n",
    "# find active nodes during this time:\n",
    "active_nodes = set(net_rw.events_table.loc[np.logical_and(\\\n",
    "               net_rw.events_table.starting_times >= time_slices[int_start][0], \n",
    "               net_rw.events_table.ending_times < time_slices[int_stop][1])].source_nodes.tolist())\n",
    "    \n",
    "active_nodes.update(net_rw.events_table.loc[np.logical_and(\\\n",
    "               net_rw.events_table.starting_times >= time_slices[int_start][0], \n",
    "               net_rw.events_table.ending_times < time_slices[int_stop][1])].target_nodes.tolist())\n",
    "\n",
    "clustres = multi_res_rw0_240[tau_w]\n",
    "clustres_rev = multi_res_rw240_600[tau_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%% plot node_class vs communities plot\n",
    "\n",
    "# sortorder = np.argsort(net_rw.node_class_array)\n",
    "# sorted_class_array = net_rw.node_class_array[sortorder]\n",
    "\n",
    "\n",
    "# sorted_active_nodes = [n for n in sortorder if n in active_nodes]\n",
    "# sorted_active_nodes_class = net_rw.node_class_array[sorted_active_nodes]\n",
    "# unique_class_array = np.unique(sorted_active_nodes_class)\n",
    "\n",
    "# #class sizes taking into account only active nodes\n",
    "# classes_size = {cla : (sorted_active_nodes_class == cla).sum() for cla in unique_class_array}\n",
    "# classes_size_list = [(sorted_active_nodes_class == cla).sum() for cla in unique_class_array]\n",
    "# assert sum(classes_size_list) == len(active_nodes)\n",
    "\n",
    "# yticks = np.cumsum(classes_size_list) - classes_size_list[0]//2\n",
    "# yticks_labels = np.unique(sorted_class_array)\n",
    "\n",
    "# class_2_number ={cla : i+1 for i, cla in enumerate(unique_class_array)}\n",
    "\n",
    "# # sort the two partition in the same way = similuraty with class partition\n",
    "# class_cluster_list = []\n",
    "# for clas in unique_class_array:\n",
    "#     class_cluster_list.append(set([n for n in active_nodes if net_rw.node_class_array[n] == clas]))\n",
    "\n",
    "# # partitions with only active nodes\n",
    "# best_part = Partition(len(active_nodes), [clust.intersection(active_nodes) for \\\n",
    "#                                       clust in clustres[0]])\n",
    "# best_part = Partition(len(active_nodes), sort_clusters(best_part.cluster_list, class_cluster_list))\n",
    "    \n",
    "# best_partrev = Partition(len(active_nodes), [clust.intersection(active_nodes) for \\\n",
    "#                                       clust in clustres_rev[0]])\n",
    "# best_partrev = Partition(len(active_nodes), sort_clusters(best_partrev.cluster_list, class_cluster_list))\n",
    "\n",
    "\n",
    "\n",
    "# node2comm = np.zeros((len(active_nodes), best_part.get_num_clusters()))\n",
    "# for i,n in enumerate(sorted_active_nodes):\n",
    "#     node2comm[i,best_part.node_to_cluster_dict[n]] = class_2_number[sorted_active_nodes_class[i]]\n",
    "\n",
    "\n",
    "# node2comm_rev = np.zeros((len(active_nodes), best_partrev.get_num_clusters()))\n",
    "# for  i,n in enumerate(sorted_active_nodes):\n",
    "#     node2comm_rev[i,best_partrev.node_to_cluster_dict[n]] = class_2_number[sorted_active_nodes_class[i]]\n",
    "\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(1,2, figsize=(13.5,6), gridspec_kw={'top':0.9, 'wspace':0.25})\n",
    "\n",
    "# ax1.imshow(node2comm, aspect='auto', cmap=cmap)\n",
    "\n",
    "# ax1.set_yticks(yticks)\n",
    "# ax1.set_yticklabels(yticks_labels)\n",
    "# ax1.set_ylim((len(active_nodes),0))\n",
    "# ax1.set_xticks(range(best_part.get_num_clusters()))\n",
    "\n",
    "# ax1.set_xlabel('communities')\n",
    "# ax1.set_ylabel('nodes')\n",
    "\n",
    "# ax2.imshow(node2comm_rev, aspect='auto', cmap=cmap)\n",
    "\n",
    "# ax2.set_yticks(yticks)\n",
    "# ax2.set_yticklabels(yticks_labels)\n",
    "# ax2.set_ylim((len(active_nodes),0))\n",
    "# ax2.set_xticks(range(best_partrev.get_num_clusters()))\n",
    "\n",
    "# ax2.set_xlabel('communities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best0_240 = np.min(NVI_forw_rw0_240)\n",
    "index_best240_600 = np.min(NVI_forw_rw240_600)\n",
    "index_best600_960 = np.min(NVI_forw_rw600_960)\n",
    "index_best960_1320 = np.min(NVI_forw_rw960_1320)\n",
    "index_best1320_1556 = np.min(NVI_forw_rw1320_1556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NVI_forw_rw0_240 == index_best0_240)\n",
    "print(lamdas_growing[NVI_forw_rw0_240 == index_best0_240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lamdas_growing[170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NVI_forw_rw240_600 == index_best240_600)\n",
    "print(lamdas_growing[NVI_forw_rw240_600 == index_best240_600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas_growing[135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NVI_forw_rw600_960 == index_best600_960)\n",
    "print(lamdas_growing[NVI_forw_rw600_960 == index_best600_960])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas_growing[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NVI_forw_rw960_1320 == index_best960_1320)\n",
    "print(lamdas_growing[NVI_forw_rw960_1320 == index_best960_1320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas_growing[125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NVI_forw_rw1320_1556 == index_best1320_1556)\n",
    "print(lamdas_growing[NVI_forw_rw1320_1556 == index_best1320_1556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas_growing[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward 240_600 Alluvial Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestcluster0 = multi_res_rw240_600[lamdas_growing[79]][0]\n",
    "bestcluster1 = multi_res_rw240_600[lamdas_growing[92]][0]\n",
    "bestcluster2 = multi_res_rw240_600[lamdas_growing[150]][0]\n",
    "bestcluster3 = multi_res_rw240_600[lamdas_growing[135]][0]\n",
    "\n",
    "bestclusters = [bestcluster0, bestcluster1, bestcluster2, bestcluster3]\n",
    "\n",
    "#%% make data for flow diagram\n",
    "\n",
    "source_comms = bestcluster0\n",
    "target_comms = bestcluster1\n",
    "class_dict = {clas : set(net_rw.node_array[net_rw.node_class_array == clas]) for \\\n",
    "                  clas in np.unique(net_rw.node_class_array)}\n",
    "\n",
    "flows = []\n",
    "for clas, clas_set in class_dict.items():\n",
    "    for s, comm_s in enumerate(source_comms):\n",
    "        for t, comm_t in enumerate(target_comms):\n",
    "            val = len(clas_set.intersection(comm_s).intersection(comm_t))\n",
    "            if val > 0:\n",
    "                flows.append({'source': s, 'target': t, 'type': clas, 'value': val})\n",
    "\n",
    "df_flows0 = pd.DataFrame.from_dict(flows)\n",
    "\n",
    "#%% make data for flow diagram\n",
    "\n",
    "source_comms = bestcluster1\n",
    "target_comms = bestcluster2\n",
    "class_dict = {clas : set(net_rw.node_array[net_rw.node_class_array == clas]) for \\\n",
    "                  clas in np.unique(net_rw.node_class_array)}\n",
    "\n",
    "flows = []\n",
    "for clas, clas_set in class_dict.items():\n",
    "    for s, comm_s in enumerate(source_comms):\n",
    "        for t, comm_t in enumerate(target_comms):\n",
    "            val = len(clas_set.intersection(comm_s).intersection(comm_t))\n",
    "            if val > 0:\n",
    "                flows.append({'source': s, 'target': t, 'type': clas, 'value': val})\n",
    "\n",
    "df_flows1 = pd.DataFrame.from_dict(flows)\n",
    "\n",
    "#%% make data for flow diagram\n",
    "\n",
    "source_comms = bestcluster2\n",
    "target_comms = bestcluster3\n",
    "class_dict = {clas : set(net_rw.node_array[net_rw.node_class_array == clas]) for \\\n",
    "                  clas in np.unique(net_rw.node_class_array)}\n",
    "\n",
    "flows = []\n",
    "for clas, clas_set in class_dict.items():\n",
    "    for s, comm_s in enumerate(source_comms):\n",
    "        for t, comm_t in enumerate(target_comms):\n",
    "            val = len(clas_set.intersection(comm_s).intersection(comm_t))\n",
    "            if val > 0:\n",
    "                flows.append({'source': s, 'target': t, 'type': clas, 'value': val})\n",
    "\n",
    "df_flows2 = pd.DataFrame.from_dict(flows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flows0['target_label'] = df_flows0['target'] + int(np.max(df_flows0['source']) + 1)\n",
    "\n",
    "df_flows1['source'] = df_flows1['source'] + int(np.max(df_flows0['source']) + 1)\n",
    "df_flows1['target_label'] = df_flows1['target'] + int(np.max(df_flows1['source']) + 1)\n",
    "\n",
    "df_flows2['source'] = df_flows2['source'] + int(np.max(df_flows1['source']) + 1)\n",
    "df_flows2['target_label'] = df_flows2['target'] + int(np.max(df_flows2['source']) + 1)\n",
    "\n",
    "df_flows = pd.concat([df_flows0, df_flows1, df_flows2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'source': list(df_flows['source']),\n",
    "    'target': list(df_flows['target_label']),\n",
    "    'value': list(df_flows['value'])\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a mapping of old labels to new labels\n",
    "label_mapping = {}\n",
    "for i in range(df_flows.shape[0]):\n",
    "    label_mapping[df_flows['source'][i]] = df_flows['source'][i]\n",
    "    label_mapping[df_flows['target_label'][i]] = df_flows['target'][i]\n",
    "\n",
    "# Create lists of unique source and target labels\n",
    "all_nodes = list(pd.concat([df_flows['source'], df_flows['target_label']]).unique())\n",
    "\n",
    "# Apply the label mapping to the node list\n",
    "all_nodes_renamed = [label_mapping[node] for node in all_nodes]\n",
    "\n",
    "# Create mapping of nodes to indices\n",
    "node_map = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "# Map source and target nodes to their indices\n",
    "df['source_id'] = df['source'].map(node_map)\n",
    "df['target_id'] = df['target'].map(node_map)\n",
    "\n",
    "# Define colors for each link\n",
    "color_list =[\"#4ba706\",\n",
    "\"#a2007e\",\n",
    "\"#806dcb\",\n",
    "\"#5eb275\",\n",
    "\"#ca3b01\",\n",
    "\"#01a4d6\",\n",
    "\"#b77600\",\n",
    "\"#a39643\",\n",
    "\"#cc6ea9\",\n",
    "\"#1e5e39\",\n",
    "\"#cb5b5a\"]\n",
    "\n",
    "dict_color = {}\n",
    "for i, df_type in enumerate(df_flows['type'].unique()):\n",
    "    dict_color[df_type] = color_list[i]\n",
    "\n",
    "\n",
    "link_color = [dict_color[i] for i in df_flows['type']]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=10,\n",
    "        thickness=1,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        #label=all_nodes_renamed \n",
    "    ),\n",
    "    link=dict(\n",
    "        source=df['source_id'],\n",
    "        target=df['target_id'],\n",
    "        value=df['value'],\n",
    "        color=link_color,\n",
    "        customdata=df_flows['type'],\n",
    "        #hovertemplate='Source: %{source.label}<br>Target: %{target.label}<br>Value: %{value}<br>Label: %{customdata}<extra></extra>'\n",
    "        hovertemplate='Value: %{value}<br> %{customdata}<extra></extra>'\n",
    "    )\n",
    ")])\n",
    "\n",
    "#fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Froward Alluvial Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestcluster0_240 = multi_res_rw0_240[lamdas_growing[170]][0]\n",
    "bestcluster240_600 = multi_res_rw240_600[lamdas_growing[135]][0]\n",
    "bestcluster600_960 = multi_res_rw600_960[lamdas_growing[180]][0]\n",
    "bestcluster960_1320 = multi_res_rw960_1320[lamdas_growing[125]][0]\n",
    "bestcluster1320_1556 = multi_res_rw1320_1556[lamdas_growing[90]][0]\n",
    "bestclusters = [bestcluster0_240, bestcluster240_600, bestcluster600_960, bestcluster960_1320, bestcluster1320_1556]\n",
    "\n",
    "#%% make data for flow diagram\n",
    "\n",
    "source_comms = bestcluster0_240\n",
    "target_comms = bestcluster240_600\n",
    "class_dict = {clas : set(net_rw.node_array[net_rw.node_class_array == clas]) for \\\n",
    "                  clas in np.unique(net_rw.node_class_array)}\n",
    "\n",
    "flows = []\n",
    "for clas, clas_set in class_dict.items():\n",
    "    for s, comm_s in enumerate(source_comms):\n",
    "        for t, comm_t in enumerate(target_comms):\n",
    "            val = len(clas_set.intersection(comm_s).intersection(comm_t))\n",
    "            if val > 0:\n",
    "                flows.append({'source': s, 'target': t, 'type': clas, 'value': val})\n",
    "\n",
    "df_flows0_600 = pd.DataFrame.from_dict(flows)\n",
    "\n",
    "#%% make data for flow diagram\n",
    "\n",
    "source_comms = bestcluster240_600\n",
    "target_comms = bestcluster600_960\n",
    "class_dict = {clas : set(net_rw.node_array[net_rw.node_class_array == clas]) for \\\n",
    "                  clas in np.unique(net_rw.node_class_array)}\n",
    "\n",
    "flows = []\n",
    "for clas, clas_set in class_dict.items():\n",
    "    for s, comm_s in enumerate(source_comms):\n",
    "        for t, comm_t in enumerate(target_comms):\n",
    "            val = len(clas_set.intersection(comm_s).intersection(comm_t))\n",
    "            if val > 0:\n",
    "                flows.append({'source': s, 'target': t, 'type': clas, 'value': val})\n",
    "\n",
    "df_flows240_960 = pd.DataFrame.from_dict(flows)\n",
    " \n",
    "#%% make data for flow diagram\n",
    "\n",
    "source_comms = bestcluster600_960\n",
    "target_comms = bestcluster960_1320\n",
    "class_dict = {clas : set(net_rw.node_array[net_rw.node_class_array == clas]) for \\\n",
    "                  clas in np.unique(net_rw.node_class_array)}\n",
    "\n",
    "flows = []\n",
    "for clas, clas_set in class_dict.items():\n",
    "    for s, comm_s in enumerate(source_comms):\n",
    "        for t, comm_t in enumerate(target_comms):\n",
    "            val = len(clas_set.intersection(comm_s).intersection(comm_t))\n",
    "            if val > 0:\n",
    "                flows.append({'source': s, 'target': t, 'type': clas, 'value': val})\n",
    "\n",
    "df_flows600_1320 = pd.DataFrame.from_dict(flows)\n",
    "\n",
    "#%% make data for flow diagram\n",
    "\n",
    "source_comms = bestcluster960_1320\n",
    "target_comms = bestcluster1320_1556\n",
    "class_dict = {clas : set(net_rw.node_array[net_rw.node_class_array == clas]) for \\\n",
    "                  clas in np.unique(net_rw.node_class_array)}\n",
    "\n",
    "flows = []\n",
    "for clas, clas_set in class_dict.items():\n",
    "    for s, comm_s in enumerate(source_comms):\n",
    "        for t, comm_t in enumerate(target_comms):\n",
    "            val = len(clas_set.intersection(comm_s).intersection(comm_t))\n",
    "            if val > 0:\n",
    "                flows.append({'source': s, 'target': t, 'type': clas, 'value': val})\n",
    "\n",
    "df_flows960_1556 = pd.DataFrame.from_dict(flows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flows0_600['target_label'] = df_flows0_600['target'] + int(np.max(df_flows0_600['source']) + 1)\n",
    "\n",
    "df_flows240_960['source'] = df_flows240_960['source'] + int(np.max(df_flows0_600['source']) + 1)\n",
    "df_flows240_960['target_label'] = df_flows240_960['target'] + int(np.max(df_flows240_960['source']) + 1)\n",
    "\n",
    "df_flows600_1320['source'] = df_flows600_1320['source'] + int(np.max(df_flows240_960['source']) + 1)\n",
    "df_flows600_1320['target_label'] = df_flows600_1320['target'] + int(np.max(df_flows600_1320['source']) + 1)\n",
    "\n",
    "df_flows960_1556['source'] = df_flows960_1556['source'] + int(np.max(df_flows600_1320['source']) + 1)\n",
    "df_flows960_1556['target_label'] = df_flows960_1556['target'] + int(np.max(df_flows960_1556['source']) + 1)\n",
    "\n",
    "df_flows = pd.concat([df_flows0_600, df_flows240_960, df_flows600_1320, df_flows960_1556], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample data\n",
    "\n",
    "list_value = df_flows['value'].copy()\n",
    "list_value[df_flows['value'] == 1] = 0.01\n",
    "\n",
    "data = {\n",
    "    'source': list(df_flows['source']),\n",
    "    'target': list(df_flows['target_label']),\n",
    "    'value': list_value\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a mapping of old labels to new labels\n",
    "label_mapping = {}\n",
    "for i in range(df_flows.shape[0]):\n",
    "    label_mapping[df_flows['source'][i]] = df_flows['source'][i]\n",
    "    label_mapping[df_flows['target_label'][i]] = df_flows['target'][i]\n",
    "\n",
    "# Create lists of unique source and target labels\n",
    "all_nodes = list(pd.concat([df_flows['source'], df_flows['target_label']]).unique())\n",
    "\n",
    "# Apply the label mapping to the node list\n",
    "all_nodes_renamed = [label_mapping[node] for node in all_nodes]\n",
    "\n",
    "# Create mapping of nodes to indices\n",
    "node_map = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "# Map source and target nodes to their indices\n",
    "df['source_id'] = df['source'].map(node_map)\n",
    "df['target_id'] = df['target'].map(node_map)\n",
    "\n",
    "# Define colors for each link\n",
    "color_list =[\"#4ba706\",\n",
    "\"#a2007e\",\n",
    "\"#806dcb\",\n",
    "\"#5eb275\",\n",
    "\"#ca3b01\",\n",
    "\"#01a4d6\",\n",
    "\"#b77600\",\n",
    "\"#a39643\",\n",
    "\"#cc6ea9\",\n",
    "\"#1e5e39\",\n",
    "\"#cb5b5a\"]\n",
    "\n",
    "dict_color = {}\n",
    "for i, df_type in enumerate(df_flows['type'].unique()):\n",
    "    dict_color[df_type] = color_list[i]\n",
    "\n",
    "\n",
    "link_color = [dict_color[i] for i in df_flows['type']]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "sankey = go.Sankey(\n",
    "    node=dict(\n",
    "        pad=10,\n",
    "        thickness=10,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        #label=all_nodes_renamed \n",
    "    ),\n",
    "    link=dict(\n",
    "        source=df['source_id'],\n",
    "        target=df['target_id'],\n",
    "        value=df['value'],\n",
    "        color=link_color,\n",
    "        customdata=df_flows['type'],\n",
    "        #hovertemplate='Source: %{source.label}<br>Target: %{target.label}<br>Value: %{value}<br>Label: %{customdata}<extra></extra>'\n",
    "        hovertemplate='Value: %{value}<br> %{customdata}<extra></extra>'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create dummy scatter traces for the legend\n",
    "\n",
    "seen = set()\n",
    "unique_link_color = [x for x in link_color if not (x in seen or seen.add(x))]\n",
    "\n",
    "legend_entries = []\n",
    "for color, label in zip(unique_link_color, df_flows['type'].unique()):\n",
    "    legend_entries.append(go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color=color),\n",
    "        legendgroup=label,\n",
    "        showlegend=True,\n",
    "        name=label\n",
    "    ))\n",
    "\n",
    "# Combine Sankey diagram and legend entries\n",
    "fig = go.Figure(data=[sankey] + legend_entries)\n",
    "\n",
    "fig.update_layout(\n",
    "    #title_text=\"Sankey Diagram with Custom Legend\",\n",
    "    font_size=10,\n",
    "    xaxis=dict(visible=False),\n",
    "    yaxis=dict(visible=False),\n",
    "    plot_bgcolor='rgba(255,255,255,1)',\n",
    "    paper_bgcolor='rgba(255,255,255,1)',\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        font=dict(size=10),  # Adjust the font size of the legend\n",
    "        itemwidth=30,        # Adjust the width of the legend items\n",
    "        itemsizing='constant',\n",
    "        traceorder='normal',\n",
    "        orientation='h',     # Arrange legend items horizontally\n",
    "        yanchor='bottom',\n",
    "        y=-0.2,              # Position legend below the plot\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=50, b=50)  # Adjust margins to fit the legend\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sankeyflow import Sankey\n",
    "\n",
    "# Sample data\n",
    "list_value = df_flows['value'].copy()\n",
    "list_value[df_flows['value'] == 1] = 1\n",
    "\n",
    "# Define the nodes and flows\n",
    "nodes = [\n",
    "    [(node, df_flows0_600[df_flows0_600['source'] == node]['value'].sum(), dict(color=\"black\")) for node in df_flows0_600['source'].unique()],\n",
    "    [(node, df_flows240_960[df_flows240_960['source'] == node]['value'].sum(), dict(color=\"black\")) for node in df_flows240_960['source'].unique()],\n",
    "    [(node, df_flows600_1320[df_flows600_1320['source'] == node]['value'].sum(), dict(color=\"black\")) for node in df_flows600_1320['source'].unique()],\n",
    "    [(node, df_flows960_1556[df_flows960_1556['source'] == node]['value'].sum(), dict(color=\"black\")) for node in df_flows960_1556['source'].unique()],\n",
    "    [(node, df_flows960_1556[df_flows960_1556['target_label'] == node]['value'].sum(), dict(color=\"black\")) for node in df_flows960_1556['target_label'].unique()],\n",
    "    #[(label_mapping[node], df_flows[df_flows['target'] == node]['value'].sum()) for node in df_flows['target_label'].unique()]\n",
    "]\n",
    "\n",
    "\n",
    "# Define colors for each link\n",
    "color_list = [\n",
    "    \"#4ba706\", \"#a2007e\", \"#806dcb\", \"#5eb275\", \"#ca3b01\",\n",
    "    \"#01a4d6\", \"#b77600\", \"#a39643\", \"#cc6ea9\", \"#1e5e39\", \"#cb5b5a\"\n",
    "]\n",
    "\n",
    "dict_color = {df_type: color_list[i] for i, df_type in enumerate(df_flows['type'].unique())}\n",
    "link_colors = [dict_color[df_flows['type'][i]] for i in range(df_flows.shape[0])]\n",
    "\n",
    "flows = [(df_flows['source'][index], df_flows['target_label'][index], df_flows['value'][index], {'color': link_colors[index]}) for index in df_flows.index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the Sankey diagram\n",
    "plt.figure(figsize=(10, 6))\n",
    "s = Sankey(flows=flows, nodes=nodes, node_opts=dict(label_format=''))\n",
    "\n",
    "# Draw the Sankey diagram\n",
    "s.draw()\n",
    "\n",
    "# Create legend\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=dict_color[df_type], markersize=10)\n",
    "           for df_type in df_flows['type'].unique()]\n",
    "\n",
    "plt.legend(handles, df_flows['type'].unique(), title='Flow Types', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "#plt.savefig(\"sankey_diagram_with_legend_sankeyflow.png\", bbox_inches='tight')\n",
    "\n",
    "# Optionally save as a different format (e.g., PDF, SVG)\n",
    "# plt.savefig(\"sankey_diagram_with_legend_sankeyflow.pdf\", bbox_inches='tight')\n",
    "#plt.savefig(\"/home/b/skoove/Desktop/primary_school/sankey_diagram_with_legend_sankeyflow.png\", bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
